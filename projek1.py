# -*- coding: utf-8 -*-
"""projek1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jQNIFx8Z4ohxdvq_6AdaarZM2zBqFZov
"""

import pandas as pd
import numpy as np

import re
import string
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.corpus import wordnet
from nltk.stem import SnowballStemmer
from nltk.stem import WordNetLemmatizer
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('avareged_perceptron_tagger')
nltk.download('wordnet')

def preprocess_text(text):
    # Cleaning: Remove URLs, special characters, and numbers
    text = re.sub(r'http\S+|www\S+|https\S+|\d+|\b\w\b', '', text, flags=re.MULTILINE)

    # Case folding: Convert to lowercase
    text = text.lower()

    # Tokenizing: Break the text into words
    tokens = word_tokenize(text)

    # Stopword removal: Remove common words (e.g., 'the', 'is', 'and')
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]

    # Stemming: Reduce words to their base or root form
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(word) for word in tokens]

    # Join tokens back into a single string
    preprocessed_text = ' '.join(tokens)

    return preprocessed_text

# Example usage
input_text = "This is an example text with some stopwords and interesting words like running."
preprocessed_text = preprocess_text(input_text)
print("Input Text: ", input_text)
print("Preprocessed Text: ", preprocessed_text)

import pandas as pd
import re

# Membaca file CSV
df_penerima = pd.read_csv('data - Sheet1.csv')

# Fungsi cleaning
clean_symbol = re.compile('[^a-zA-Z\s]')  # Updated regex to keep letters and spaces only

def cleaning(text):
    text = re.sub('@[^\s]+', ' ', text)
    text = re.sub('#[^\s]+', ' ', text)
    text = re.sub(r"(?:\@|http?\://|https?\://|www)\S+", "", text)
    text = re.sub(r"\d+", "", text)
    text = text.lower()
    text = text.strip(" ")
    text = clean_symbol.sub(' ', text)
    return text

# Menggunakan fungsi cleaning pada kolom 'content'
df_penerima['content'] = df_penerima['content'].apply(cleaning)

# Menampilkan hasil cleaning dari kolom 'content'
print(df_penerima['content'])

# Save the updated DataFrame to a new CSV file
df_penerima.to_csv('data - Sheet1_Cleaned.csv', index=False)

import pandas as pd
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.corpus import stopwords
",".join(stopwords.words('indonesian'))
stop_words=set(stopwords.words('indonesian'))



def remove_stop(x):
  return " ".join([word for word in str(x).split() if word not in stop_words])

def apply_stopword_removal_on_column(df, column_name):
    # Terapkan fungsi remove stop pada kolom tertentu
    df[column_name] = df[column_name].apply(remove_stop)
    return df

file_path = 'data - Sheet1_Cleaned.csv'

# Baca file CSV
data = pd.read_csv(file_path)

# Terapkan stopword removal pada kolom 'content'
data = apply_stopword_removal_on_column(data, 'content')

# Simpan data yang telah diubah kembali ke file CSV
data.to_csv('data - Sheet1_stopword.csv', index=False)

print("Stopword removal selesai. Hasil disimpan dalam file 'data - Sheet1_stopword.csv'.")

data = pd.read_csv('data - Sheet1_stopword.csv')
data.head()

import pandas as pd
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')

def tokenizing(text):
    return word_tokenize(text)

def tokenizing_column(df, column_name):
    # tokenizing
    df[column_name] = df[column_name].apply(tokenizing)
    return df

file_path = 'data - Sheet1_stopword.csv'

# Baca file CSV
data = pd.read_csv(file_path)

# Terapkan token pada kolom 'content'
data = tokenizing_column(data, 'content')

# Simpan data yang telah diubah kembali ke file CSV
data.to_csv('data - Sheet1_tokenizing.csv', index=False)

print("Tokenizing selesai. Hasil disimpan dalam file 'data - Sheet1_tokenizing.csv'.")

data = pd.read_csv('data - Sheet1_tokenizing.csv')
data.head()

pip install sastrawi

pip freeze

import pandas as pd
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

def stemming(text):
    stemmer_factory = StemmerFactory()
    stemmer = stemmer_factory.create_stemmer()

    # Melakukan stemming pada text
    stemmed_text = stemmer.stem(text)

    return stemmed_text

def main(df, column_name):
    # Melakukan stemming pada kolom 'content'
    df['content'] = df['content'].apply(stemming)
    return df

# Ganti 'nama_file_input.csv' dengan nama file CSV yang Anda miliki
input_file_path = 'data - Sheet1_tokenizing.csv'

# Membaca file CSV
data = pd.read_csv(input_file_path)

data = main(data, 'content')

# Menyimpan DataFrame yang telah diubah ke file CSV baru
data.to_csv('data - Sheet1_stemmed.csv', index=False)

print("Stemming selesai. Hasil disimpan ke: 'data - Sheet1_stemmed.csv'")

data = pd.read_csv('data - Sheet1_stemmed.csv')
data.head()

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from math import log

# Function for TF calculation
def hitung_TF(document):
    tf_dict = {}
    banyak_document = len(document)

    # Menghitung jumlah kemunculan kata di setiap dokumen
    for word in document:
        if word in tf_dict:
            tf_dict[word] += 1
        else:
            tf_dict[word] = 1

    # Menghitung nilai TF
    for word in tf_dict:
        tf_dict[word] = tf_dict[word] / banyak_document

    return tf_dict

# Function for DF calculation
def hitung_DF(documents):
    count_DF = {}

    # Menghitung dokumen yang mengandung kata
    for document in documents:
        unique_words = set(document)
        for word in unique_words:
            if word in count_DF:
                count_DF[word] += 1
            else:
                count_DF[word] = 1

    return count_DF

# Function for IDF calculation
def hitung_IDF(banyak_dokumen, count_DF):
    IDF_dict = {}

    for word in count_DF:
        IDF_dict[word] = log(banyak_dokumen / (count_DF[word] + 1))

    return IDF_dict

# Function for TF-IDF calculation
def hitung_TFIDF(TF, IDF):
    TF_IDF_dict = {}

    for word in TF:
        TF_IDF_dict[word] = TF[word] * IDF[word]

    return TF_IDF_dict

# Ganti 'data - Sheet1.csv' dengan nama file CSV hasil preprocessing Anda
input_file_path = 'data - Sheet1_stemmed.csv'

# Baca file CSV
data = pd.read_csv(input_file_path)

# Membuat dokumen dari kolom yang telah dipreprocessing
documents = data['content'].apply(lambda text: text.split())

# Menghitung TF untuk setiap dokumen
TF_list = documents.apply(hitung_TF)

# Menghitung DF
count_DF = hitung_DF(documents)

# Menghitung IDF
banyak_dokumen = len(documents)
IDF_dict = hitung_IDF(banyak_dokumen, count_DF)

# Menghitung TF-IDF
data['TFIDF'] = TF_list.apply(lambda TF: hitung_TFIDF(TF, IDF_dict))

# Save the DataFrame with TF-IDF values to a new CSV file
data.to_csv('TFIDF_results.csv', index=False)

print(f"TF-IDF values have been calculated and saved to 'TFIDF_results.csv'")

data = pd.read_csv('TFIDF_results.csv')
data.head()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# Baca file CSV yang berisi kolom content, label, dan tf-idf
file_path = 'TFIDF_results.csv'
data = pd.read_csv(file_path)

# Kolom target (label) - Sesuaikan dengan nama kolom yang berisi label klasifikasi pada data Anda
target_column = 'Label'

# Bagi data menjadi set pelatihan dan set pengujian
X_train, X_test, y_train, y_test = train_test_split(data['TFIDF'], data[target_column], test_size=0.2, random_state=42)

# Inisialisasi TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer()

# Transformasi teks ke matriks TF-IDF
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Inisialisasi model Naive Bayes
naive_bayes_model = MultinomialNB()

# Melatih model Naive Bayes menggunakan set pelatihan
naive_bayes_model.fit(X_train_tfidf, y_train)

# Memprediksi label untuk set pengujian
predictions = naive_bayes_model.predict(X_test_tfidf)

# Evaluasi performa model
accuracy = accuracy_score(y_test, predictions)
classification_rep = classification_report(y_test, predictions)

# Tampilkan hasil evaluasi
print(f"Accuracy: {accuracy}")
print("Classification Report:")
print(classification_rep)
